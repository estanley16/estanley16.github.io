<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | emma stanley </title> <meta name="author" content="Emma Stanley"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%BF&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://estanley16.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> emma stanley </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2026</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> </div> <div id="stanley_synthetic_2026" class="col-sm-8"> <div class="title">Synthetic Ground Truth Counterfactuals for Comprehensive Evaluation of Causal Generative Models in Medical Imaging</div> <div class="author"> <em>Emma A. M. Stanley</em>, Vibujithan Vigneshwaran, Erik Y. Ohara, Finn G. Vamosi, Nils D. Forkert, and Matthias Wilms </div> <div class="periodical"> <em>In Medical Image Computing and Computer Assisted Intervention – MICCAI 2025</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-032-04984-1_52" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Counterfactuals in medical imaging are synthetic representations of how an individual’s medical image might appear under alternative, typically unobservable conditions, which have the potential to address data limitations and enhance interpretability. However, counterfactual images, which can be generated by causal generative models (CGMs), are inherently hypothetical—raising questions of how to properly validate that they are realistic and accurately reflect the intended modifications. A common approach for quantitatively evaluating CGM-generated counterfactuals involves using a discriminative model as a ‘pseudo-oracle’ to assess whether interventions on specific variables are effective. However, this method is not well-suited for in-depth error identification and analysis of CGMs. To address this limitation, we propose to leverage synthetic, ‘ground truth’ counterfactual datasets as a novel approach for debugging and evaluating CGMs. These synthetic datasets enable the computation of global performance metrics and precise localization of CGM failure modes. To further quantify failures, we introduce a novel metric, the Triangulation of Effectiveness and Amplification (TEA), which precisely quantifies the effectiveness of target variable interventions and the additional amplification of unintended effects. We test and validate our evaluation framework on two state-of-the-art CGMs where the results demonstrate the utility of synthetic datasets in identifying failure modes of CGMs, and highlight the potential of the proposed TEA metric as a robust tool for evaluation of their performance. Code and data are available at https://github.com/ucalgary-miplab/TEA.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI WS</abbr> </div> <div id="stanley_exploring_2026" class="col-sm-8"> <div class="title">Exploring the Interplay of Label Bias with Subgroup Size and Separability: A Case Study in Mammographic Density Classification</div> <div class="author"> <em>Emma A. M. Stanley</em>, Raghav Mehta, Mélanie Roschewitz, Nils D. Forkert, and Ben Glocker </div> <div class="periodical"> <em>In Fairness of AI in Medical Imaging</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-032-05870-6_8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Systematic mislabelling affecting specific subgroups (i.e., label bias) in medical imaging datasets represents an understudied issue concerning the fairness of medical AI systems. In this work, we investigated how size and separability of subgroups affected by label bias influence the learned features and performance of a deep learning model. Therefore, we trained deep learning models for binary tissue density classification using the EMory BrEast imaging Dataset (EMBED), where label bias affected separable subgroups (based on imaging manufacturer) or non-separable ‘pseudo-subgroups’. We found that simulated subgroup label bias led to prominent shifts in the learned feature representations of the models. Importantly, these shifts within the feature space were dependent on both the relative size and the separability of the subgroup affected by label bias. We also observed notable differences in subgroup performance depending on whether a validation set with clean labels was used to define the classification threshold for the model. For instance, with label bias affecting the majority separable subgroup, the true positive rate for that subgroup fell from 0.898, when the validation set had clean labels, to 0.518, when the validation set had biased labels. Our work represents a key contribution toward understanding the consequences of label bias on subgroup fairness in medical imaging AI.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI WS</abbr> </div> <div id="gillett_simulating_2026" class="col-sm-8"> <div class="title">Simulating Inter-observer Variability Across Clinical Experience Levels for Brain Tumour Segmentation</div> <div class="author"> Haley Gillett, <em>Emma A. M. Stanley</em>, Raissa Souza, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>In Human-AI Collaboration</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-032-08970-0_8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Human-AI collaboration is essential for the development of trustworthy deep learning (DL) models for medical image analysis. However, datasets annotated by multiple clinical experts can introduce inter-observer variability, which can then give rise to annotator biases that may be learned by the DL model. Assessment of these biases is often hindered by the limited availability of multi-observer annotations for the same datasets. To address this limitation, we present a novel simulation framework that generates realistic variations in annotated segmentations to mimic inter-observer differences across simulated human experts with varying experience levels. Using brain tumour segmentation as a representative case study, we simulated three observer labels to train DL models. Our results show that DL models learn observer-specific annotation styles. For example, models trained on the data from a simulated senior radiologist with a tendency to under-segment the tumour tissue achieved higher performance than those trained on over-segmented ones. Inter-observer agreement was not strictly correlated with experience levels nor downstream DL model performance, demonstrating the complexity of annotation biases. Additionally, datasets with single ground-truth labels may mask important differences from learned annotation bias and over- or underestimate model performance. Human-AI collaboration, although necessary for medical imaging tasks, can introduce biases that negatively affect model segmentation performance and may undermine fairness, trust, and transparency. Our study takes an essential step toward understanding these risks and provides insights that support the development of human–AI collaborative systems designed for real-world clinical applicability.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="casillas2025interpretable" class="col-sm-8"> <div class="title">Interpretable convolutional neural network for autism diagnosis support in children using structural magnetic resonance imaging datasets</div> <div class="author"> Garazi Casillas Martinez, Anthony Winder, <em>Emma A. M. Stanley</em>, Raissa Souza, Matthias Wilms, Myka Estes, Sarah J MacEachern, and Nils D Forkert </div> <div class="periodical"> <em>Journal of Medical Imaging</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="stanley_where_2025" class="col-sm-8"> <div class="title">Where, why, and how is bias learned in medical image analysis models? A study of bias encoding within convolutional networks using synthetic data</div> <div class="author"> <em>Emma A. M. Stanley</em>, Raissa Souza, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>eBioMedicine</em>, Jan 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.ebiom.2024.105501" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Background Understanding the mechanisms of algorithmic bias is highly challenging due to the complexity and uncertainty of how various unknown sources of bias impact deep learning models trained with medical images. This study aims to bridge this knowledge gap by studying where, why, and how biases from medical images are encoded in these models. Methods We systematically studied layer-wise bias encoding in a convolutional neural network for disease classification using synthetic brain magnetic resonance imaging data with known disease and bias effects. We quantified the degree to which disease-related information, as well as morphology-based and intensity-based biases were represented within the learned features of the model. Findings Although biases were encoded throughout the model, a stronger encoding did not necessarily lead to the model using these biases as a shortcut for disease classification. We also observed that intensity-based effects had a greater influence on shortcut learning compared to morphology-based effects when multiple biases were present. Interpretation We believe that these results constitute an important first step towards a deeper understanding of algorithmic bias in deep learning models trained using medical imaging data. This study also showcases the benefits of utilising controlled, synthetic bias scenarios for objectively studying the mechanisms of shortcut learning. Funding Alberta Innovates, Natural Sciences and Engineering Research Council of Canada, Killam Trusts, Parkinson Association of Alberta, River Fund at Calgary Foundation, Canada Research Chairs Program.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI WS</abbr> </div> <div id="stanley_assessing_2025" class="col-sm-8"> <div class="title">Assessing the Impact of Sociotechnical Harms in AI-Based Medical Image Analysis</div> <div class="author"> <em>Emma A. M. Stanley</em>, Raissa Souza, Anthony J. Winder, Matthias Wilms, G. Bruce Pike, Gabrielle Dagasso, Christopher Nielsen, Sarah J. MacEachern, and Nils D. Forkert </div> <div class="periodical"> <em>In Ethics and Fairness in Medical Imaging</em>, Jan 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-72787-0_16" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Clinical decision-making and radiology will inevitably be transformed by artificial intelligence (AI) in the coming years. The rapid adoption of AI in this domain, and in other everyday applications, has brought an increased awareness of the potential impacts and negative consequences that may occur throughout the sociotechnical systems that these technologies are implemented in. In this paper, we review and apply a previously published taxonomy of the sociotechnical harms of AI to investigate how these harms could manifest during the development and clinical implementation of AI-based medical image analysis. Through an illustrative case study example on computer-aided diagnosis using brain magnetic resonance imaging, we demonstrate how performing impact assessments of sociotechnical harms can assist in operationalizing the medical ethics principle of non-maleficence, thereby guiding the ethical development and implementation of AI technologies in healthcare.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MIDL</abbr> </div> <div id="vigneshwaran_evaluating_2025" class="col-sm-8"> <div class="title">Evaluating Shortcut Utilization in Deep Learning Disease Classification through Counterfactual Analysis</div> <div class="author"> Vibujithan Vigneshwaran, <em>Emma A. M. Stanley</em>, Raissa Souza, Erik Ohara, Matthias Wilms, and Nils Forkert </div> <div class="periodical"> <em>In Medical Imaging with Deep Learning</em>, Jan 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Although deep learning models can surpass human performance in many medical image analysis tasks, they remain vulnerable to algorithmic shortcuts, where spurious correlations in the data are exploited, which may lead to reduced trust in their predictions/classifications. This issue is especially concerning when models rely on protected attributes (e.g., sex, race, or site) as shortcuts. Such shortcut reliance not only impairs their ability to generalize to unseen datasets but also raises fairness concerns, ultimately undermining their purpose for computer-aided diagnosis. Previous techniques for analyzing protected attributes, such as supervised prediction layer information tests, only highlight the presence of protected attributes in the feature space but do not confirm their role in solving the primary task. Determining the impact of protected attributes as shortcuts is particularly challenging, as it requires knowing how a model would perform without those attributes — a counterfactual scenario typically unattainable in real-world data. As a workaround, researchers have addressed the absence of counterfactuals by generating synthetic datasets with and without protected attributes. In this study, we propose a novel approach to evaluate real-world datasets and determine the extent to which each protected attribute is used as a shortcut in a classification task. Therefore, we define and train a causal generative model to produce causally-grounded counterfactuals, removing protected attributes from activations and allowing us to measure their impact on model performance. Employing T1-weighted MRI data from 9 sites (835 subjects: 426 with Parkinson’s disease (PD) and 409 healthy), we demonstrate that counterfactually removing the ’site’ attribute from the penultimate layer of a trained classification model reduced the AUROC for PD classification from 0.74 to 0.65, indicating a 9% performance improvement achieved by using ’site’ as a shortcut. In contrast, counterfactually removing the ’sex’ attribute had minimal impact on performance, with only a slight change of 0.004, indicating that ’sex’ was not utilized as a shortcut by the classification model. The proposed method offers a robust framework for assessing shortcut utilization in medical image classification, paving the way for improved bias detection and mitigation in medical imaging tasks. The code for this work is available on https://github.com/vibujithan/shortcut-analysis.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SPIE</abbr> </div> <div id="stanley_does_2025" class="col-sm-8"> <div class="title">Does a diffusion-based generative classifier avoid shortcut learning in medical image analysis? An initial investigation using synthetic neuroimaging data</div> <div class="author"> <em>Emma A. M. Stanley</em>, Nils D. Forkert, and Matthias Wilms </div> <div class="periodical"> <em>In Medical Imaging 2025: Imaging Informatics</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1117/12.3046811" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Powerful deep learning-based classification models have led to impressive performance gains of computer-aided diagnosis systems in medical imaging over the last decade. However, the training of such discriminative models usually requires a significant amount of labelled training images that are representative of the data the model will see after (clinical) deployment. Acquiring such data is often highly challenging and costly in medical image analysis setups, which may lead to so-called shortcut learning where the model learns a relationship in the data that is not causally related to the actual classification task. Biased models are known to be less robust and may act unfairly when applied to data from subpopulations underrepresented in the training data, resulting in ethical challenges when deployed clinically. Recent results from the machine learning domain have shown that generative classifiers (i.e., a generative model repurposed as a classifier) might be less prone to this shortcut learning behavior than standard discriminative classifiers. Motivated by those previous results, this work provides the first-ever analysis of the shortcut learning behavior of a diffusion model-based generative classifier for medical image analysis. Our investigation relies on a state-of-the-art framework for synthetic neuroimage data generation with customizable biases. The initial results obtained using this data showcase that while our generative classifier still relies on shortcuts to render its decision to a certain extent, the degree of shortcut learning is substantially reduced compared to a standard discriminative classifier - a convolutional neural network - trained on identical data. We believe that these results highlight the advantages generative classifiers could potentially have on a path towards more trustworthy and equitable AI in medical image analysis.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SPIE</abbr> </div> <div id="martinez_explainable_2025" class="col-sm-8"> <div class="title">Explainable classification of autism in children with a convolutional neural network</div> <div class="author"> Garazi Casillas Martinez, Anthony Winder, <em>Emma A. M. Stanley</em>, Raissa Souza, Matthias Wilms, Myka Estes, Sarah J. MacEachern, and Nils D. Forkert </div> <div class="periodical"> <em>In Medical Imaging 2025: Imaging Informatics</em>, Apr 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1117/12.3047066" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Autism is a complex neurodevelopmental condition that influences how individuals interact, communicate, and behave. Although its prevalence is high, it can be challenging to make an early diagnosis, mainly due to the gradual onset patterns of its symptoms. Artificial intelligence (AI) models using magnetic resonance imaging (MRI) can support the diagnosis of autism in children by detecting complex disease-related brain patterns that are not obvious to human experts. The purpose of this study was to develop and evaluate an explainable deep learning (DL) model to support the diagnosis of autism in children and to identify the most important brain regions for the classification task. For the development and evaluation of the proposed DL model, we used 452 T1-weighted structural magnetic resonance images of individuals aged 9 to 11 years from the Autism Brain Imaging Data Exchange I and II (ABIDE I and II) databases. Using this data sample, a convolutional neural network was trained to classify neurologically typical children and autistic children (360 used for training / 46 images for validation / 46 images for testing). The results based on the images used as the test set showed that the proposed deep learning method achieves an overall accuracy of 71.74%, with a sensitivity of 73.91% and a specificity of 70.83%. The corresponding saliency voxel attribution maps were computed, which hihglighted the left transverse temporal gyrus, the left lateral ventricle, the left VI-VII vermal lobules, and the left thalamus as the most important regions for the classification task. These brain regions are consistent with previous studies that identified differences in these areas in autistic individuals. To our knowledge, this is the first study that aims to classify autism in children aged 9-11 years using a deep learning approach based on structural MRI data in combination with artificial intelligence explainability techniques to identify relevant brain regions for this task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="mcavoy_brain_2025" class="col-sm-8"> <div class="title">Brain Aging in Patients With Cardiovascular Disease From the UK Biobank</div> <div class="author"> Elizabeth Mcavoy, <em>Emma A. M. Stanley</em>, Anthony J. Winder, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>Human Brain Mapping</em>, Apr 2025 </div> <div class="periodical"> _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.70252 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1002/hbm.70252" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The brain undergoes complex but normal structural changes during the aging process in healthy adults, whereas deviations from the normal aging patterns of the brain can be indicative of various conditions as well as an increased risk for the development of diseases. The brain age gap (BAG), which is defined as the difference between the chronological age and the machine learning-predicted biological age of an individual, is a promising biomarker for determining whether an individual deviates from normal brain aging patterns. While the BAG has shown promise for various neurological diseases and cardiovascular risk factors, its utility to quantify brain changes associated with diagnosed cardiovascular diseases has not been investigated to date, which is the aim of this study. T1-weighted MRI scans from healthy participants in the UK Biobank were used to train a convolutional neural network (CNN) model for biological brain age prediction. The trained model was then used to quantify and compare the BAGs for all participants in the UK Biobank with known cardiovascular diseases, as well as healthy controls and patients with known neurological diseases for benchmark comparisons. Saliency maps were computed for each individual to investigate whether brain regions used for biological brain age prediction by the CNN differ between groups. The analyses revealed significant differences in BAG distributions for 10 of the 42 sex-specific cardiovascular disease groups investigated compared to healthy participants, indicating disease-specific variations in brain aging. However, no significant differences were found regarding the brain regions used for brain age prediction as determined by saliency maps, indicating that the model mostly relied on healthy brain aging patterns, even in the presence of cardiovascular diseases. Overall, the findings of this work demonstrate that the BAG is a sensitive imaging biomarker to detect differences in brain aging associated with specific cardiovascular diseases. This further supports the theory of the heart–brain axis by exemplifying that many cardiovascular diseases are associated with atypical brain aging.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="nielsen_assessment_2025" class="col-sm-8"> <div class="title">Assessment of demographic bias in retinal age prediction machine learning models</div> <div class="author"> Christopher Nielsen, <em>Emma A. M. Stanley</em>, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>Frontiers in Artificial Intelligence</em>, Oct 2025 </div> <div class="periodical"> Publisher: Frontiers </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3389/frai.2025.1653153" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The retinal age gap, defined as the difference between the predicted retinal age and chronological age, is an emerging biomarker for many eye conditions and even non-ocular diseases. Machine learning (ML) models are commonly used for retinal age prediction. However, biases in ML models may lead to unfair predictions for some demographic groups, potentially exacerbating health disparities. This retrospective cross-sectional study evaluated demographic biases related to sex and ethnicity in retinal age prediction models using retinal imaging data (color fundus photography [CFP], optical coherence tomography [OCT], and combined CFP + OCT) from 9,668 healthy individuals (mean age 56.8 years; 52% female) in the UK Biobank. The RETFound foundation model was fine-tuned to predict retinal age, and bias was assessed by comparing mean absolute error (MAE) and retinal age gaps across demographic groups. The combined CFP + OCT model achieved the lowest MAE (3.01 years), outperforming CFP-only (3.40 years) and OCT-only (4.37 years) models. Significant sex differences were observed only in the CFP model (p \textless 0.001), while significant ethnicity differences appeared only in the OCT model (p \textless 0.001). No significant sex/ethnicity differences were observed in the combined model. These results demonstrate that retinal age prediction models can exhibit biases, and that these biases, along with model accuracy, are influenced by the choice of imaging modality (CFP, OCT, or combined). Identifying and addressing sources of bias is essential for safe and reliable clinical implementation. Our results emphasize the importance of comprehensive bias assessments and prospective validation, ensuring that advances in machine learning and artificial intelligence benefit all patient populations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="souza_self-supervised_2025" class="col-sm-8"> <div class="title">Self-supervised identification and elimination of harmful datasets in distributed machine learning for medical image analysis</div> <div class="author"> Raissa Souza, <em>Emma A. M. Stanley</em>, Anthony J. Winder, Chris Kang, Kimberly Amador, Erik Y. Ohara, Gabrielle Dagasso, Richard Camicioli, Oury Monchi, Zahinoor Ismail, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>npj Digital Medicine</em>, Feb 2025 </div> <div class="periodical"> Publisher: Nature Publishing Group </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1038/s41746-025-01499-0" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Distributed learning enables collaborative machine learning model training without requiring cross-institutional data sharing, thereby addressing privacy concerns. However, local quality control variability can negatively impact model performance while systematic human visual inspection is time-consuming and may violate the goal of keeping data inaccessible outside acquisition centers. This work proposes a novel self-supervised method to identify and eliminate harmful data during distributed learning model training fully-automatically. Harmful data is defined as samples that, when included in training, increase misdiagnosis rates. The method was tested using neuroimaging data from 83 centers for Parkinson’s disease classification with simulated inclusion of a few harmful data samples. The proposed method reliably identified harmful images, with centers providing only harmful datasets being easier to identify than single harmful images within otherwise good datasets. While only evaluated using neuroimaging data, the presented method is application-agnostic and presents a step towards automated quality control in distributed learning.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="moore_towards_2025" class="col-sm-8"> <div class="title">Towards realistic simulation of disease progression in the visual cortex with CNNs</div> <div class="author"> Jasmine A. Moore, Chris Kang, Vibujithan Vigneshwaran, <em>Emma A. M. Stanley</em>, Ashar Memon, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>Scientific Reports</em>, Feb 2025 </div> <div class="periodical"> Publisher: Nature Publishing Group </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1038/s41598-025-89738-y" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Convolutional neural networks (CNNs) and mammalian visual systems share architectural and information processing similarities. We leverage these parallels to develop an in-silico CNN model simulating diseases affecting the visual system. This model aims to replicate neural complexities in an experimentally controlled environment. Therefore, we examine object recognition and internal representations of a CNN under neurodegeneration and neuroplasticity conditions simulated through synaptic weight decay and retraining. This approach can model neurodegeneration from events like tau accumulation, reflecting cognitive decline in diseases such as posterior cortical atrophy, a condition that can accompany Alzheimer’s disease and primarily affects the visual system. After each degeneration iteration, we retrain unaffected synapses to simulate ongoing neuroplasticity. Our results show that with significant synaptic decay and limited retraining, the model’s representational similarity decreases compared to a healthy model. Early CNN layers retain high similarity to the healthy model, while later layers are more prone to degradation. The results of this study reveal a progressive decline in object recognition proficiency, mirroring posterior cortical atrophy progression. In-silico modeling of neurodegenerative diseases can enhance our understanding of disease progression and aid in developing targeted rehabilitation and treatments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI WS</abbr> </div> <div id="souza_sites_2025" class="col-sm-8"> <div class="title">Do Sites Benefit Equally from Distributed Learning in Medical Image Analysis?</div> <div class="author"> Raissa Souza, <em>Emma A. M. Stanley</em>, Richard Camicioli, Oury Monchi, Zahinoor Ismail, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>In Ethics and Fairness in Medical Imaging</em>, Feb 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-72787-0_12" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Artificial intelligence (AI) has the potential to make medical image analysis more accessible to healthcare institutions worldwide. However, when trained on multi-site datasets, models may excel with data from certain institutions but struggle with others, even when exposed to their training data. This emphasizes the importance of investigating whether all sites benefit from AI models, especially within distributed learning setups. Distributed learning methods allow access to broader and more diverse datasets from multiple sites during training. In this context, the travelling model (TM) paradigm has demonstrated superior performance in limited data scenarios, making it particularly relevant in low-resource settings. This work investigates whether all sites can benefit from TM development and identifies the factors influencing performance disparities. Specifically, a Parkinson’s disease (PD) database comprising 1,817 neuroimaging datasets from 83 different sites is utilized to investigate how site-specific and participant-specific factors influence the performance of TM in classifying PD. Therefore, we analyze the false positive rate (FPR) and false negative rate (FNR) to identify the characteristics contributing to misdiagnosis. Our findings reveal disparities in benefits across sites, with class imbalance emerging as the major factor influencing FPR and FNR, especially for sites with more PD cases. This research underscores the urgency of a rigorous analysis of a model’s behaviour in distributed setups to detect misdiagnosis risks and encourage developers to evaluate and optimize models beyond overall accuracy. Thus, comprehensive analyses of this type can help pave the way for the development of more equitable AI-based medical imaging models.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI WS</abbr> </div> <div id="wilms2024lightweight" class="col-sm-8"> <div class="title">A lightweight 3D conditional diffusion model for self-explainable brain age prediction in adults and children</div> <div class="author"> Matthias Wilms, Ahmad O Ahsan, Erik Y Ohara, Gabrielle Dagasso, Elizabeth Macavoy, <em>Emma A. M. Stanley</em>, Vibujithan Vigneshwaran, and Nils D Forkert </div> <div class="periodical"> <em>In International Workshop on Machine Learning in Clinical Neuroimaging</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="romme2024analysis" class="col-sm-8"> <div class="title">Analysis and visualization of the effect of multiple sclerosis on biological brain age</div> <div class="author"> Catharina J. A. Romme, <em>Emma A. M. Stanley</em>, Pauline Mouches, Matthias Wilms, G Bruce Pike, Luanne M Metz, and Nils D Forkert </div> <div class="periodical"> <em>Frontiers in Neurology</em>, Feb 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="souza_identifying_2024" class="col-sm-8"> <div class="title">Identifying biases in a multicenter MRI database for Parkinson’s disease classification: Is the disease classifier a secret site classifier?</div> <div class="author"> Raissa Souza, Anthony Winder, <em>Emma A. M. Stanley</em>, Vibujithan Vigneshwaran, Milton Camacho, Richard Camicioli, Oury Monchi, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>IEEE Journal of Biomedical and Health Informatics</em>, Feb 2024 </div> <div class="periodical"> Conference Name: IEEE Journal of Biomedical and Health Informatics </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/JBHI.2024.3352513" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Sharing multicenter imaging datasets can be advantageous to increase data diversity and size but may lead to spurious correlations between site-related biological and non-biological image features and target labels, which machine learning (ML) models may exploit as shortcuts. To date, studies analyzing how and if deep learning models may use such effects as a shortcut are scarce. Thus, the aim of this work was to investigate if site-related effects are encoded in the feature space of an established deep learning model designed for Parkinson’s disease (PD) classification based on T1-weighted MRI datasets. Therefore, all layers of the PD classifier were frozen, except for the last layer of the network, which was replaced by a linear layer that was exclusively re-trained to predict three potential bias types (biological sex, scanner type, and originating site). Our findings based on a large database consisting of 1880 MRI scans collected across 41 centers show that the feature space of the established PD model (74% accuracy) can be used to classify sex (75% accuracy), scanner type (79% accuracy), and site location (71% accuracy) with high accuracies despite this information never being explicitly provided to the PD model during original training. Overall, the results of this study suggest that trained image-based classifiers may use unwanted shortcuts that are not meaningful for the actual clinical task at hand. This finding may explain why many image-based deep learning models do not perform well when applied to data from centers not contributing to the training set</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="souza_multi-center_2024" class="col-sm-8"> <div class="title">A multi-center distributed learning approach for Parkinson’s disease classification using the traveling model paradigm</div> <div class="author"> Raissa Souza, <em>Emma A. M. Stanley</em>, Milton Camacho, Richard Camicioli, Oury Monchi, Zahinoor Ismail, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>Frontiers in Artificial Intelligence</em>, Feb 2024 </div> <div class="periodical"> Publisher: Frontiers </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3389/frai.2024.1301997" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Distributed learning is a promising alternative to central learning for machine learning (ML) model training, overcoming data-sharing problems in healthcare. Previous studies exploring federated learning (FL) or the travelling model (TM) setup for medical image-based disease classification often relied on large databases with a limited number of centers or simulated artificial centers, raising doubts about real-world applicability. This study develops and evaluates a convolution neural network (CNN) for Parkinson’s disease classification using data acquired by 83 diverse real centers around the world, mostly contributing small training samples. Our 1 Souza et al.approach specifically makes use of the TM setup, which has proven effective in scenarios with limited data availability but has never been used for image-based disease classification. Our findings reveal that TM is effective for training CNN models, even in complex real-world scenarios with variable data distributions. After sufficient training cycles, the TM-trained CNN matches or slightly surpasses the performance of the centrally trained counterpart (AUROC of 83% vs. 80%).Our study highlights, for the first time, the effectiveness of TM in 3D medical image classification, especially in scenarios with limited training samples and heterogeneous distributed data. These insights are relevant for situations where ML models are supposed to be trained using data from small or remote medical centers, and rare diseases with sparse cases. The simplicity of this approach enables a broad application to many deep learning tasks, enhancing its clinical utility across various contexts and medical facilities.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="stanley_towards_2024" class="col-sm-8"> <div class="title">Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging</div> <div class="author"> <em>Emma A. M. Stanley</em>, Raissa Souza, Anthony J Winder, Vedant Gulve, Kimberly Amador, Matthias Wilms, and Nils D Forkert </div> <div class="periodical"> <em>Journal of the American Medical Informatics Association</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1093/jamia/ocae165" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Artificial intelligence (AI) models trained using medical images for clinical tasks often exhibit bias in the form of subgroup performance disparities. However, since not all sources of bias in real-world medical imaging data are easily identifiable, it is challenging to comprehensively assess their impacts. In this article, we introduce an analysis framework for systematically and objectively investigating the impact of biases in medical images on AI models.Our framework utilizes synthetic neuroimages with known disease effects and sources of bias. We evaluated the impact of bias effects and the efficacy of 3 bias mitigation strategies in counterfactual data scenarios on a convolutional neural network (CNN) classifier.The analysis revealed that training a CNN model on the datasets containing bias effects resulted in expected subgroup performance disparities. Moreover, reweighing was the most successful bias mitigation strategy for this setup. Finally, we demonstrated that explainable AI methods can aid in investigating the manifestation of bias in the model using this framework.The value of this framework is showcased in our findings on the impact of bias scenarios and efficacy of bias mitigation in a deep learning model pipeline. This systematic analysis can be easily expanded to conduct further controlled in silico trials in other investigations of bias in medical imaging AI.Our novel methodology for objectively studying bias in medical imaging AI can help support the development of clinical decision-support tools that are robust and responsible.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="souza_harmonytm_2024" class="col-sm-8"> <div class="title">HarmonyTM: multi-center data harmonization applied to distributed learning for Parkinson’s disease classification</div> <div class="author"> Raissa Souza, <em>Emma A. M. Stanley</em>, Vedant Gulve, Jasmine Moore, Chris Kang, Richard Camicioli, Oury Monchi, Zahinoor Ismail, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>Journal of Medical Imaging</em>, Sep 2024 </div> <div class="periodical"> Publisher: SPIE </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1117/1.JMI.11.5.054502" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>PurposeDistributed learning is widely used to comply with data-sharing regulations and access diverse datasets for training machine learning (ML) models. The traveling model (TM) is a distributed learning approach that sequentially trains with data from one center at a time, which is especially advantageous when dealing with limited local datasets. However, a critical concern emerges when centers utilize different scanners for data acquisition, which could potentially lead models to exploit these differences as shortcuts. Although data harmonization can mitigate this issue, current methods typically rely on large or paired datasets, which can be impractical to obtain in distributed setups.ApproachWe introduced HarmonyTM, a data harmonization method tailored for the TM. HarmonyTM effectively mitigates bias in the model’s feature representation while retaining crucial disease-related information, all without requiring extensive datasets. Specifically, we employed adversarial training to “unlearn” bias from the features used in the model for classifying Parkinson’s disease (PD). We evaluated HarmonyTM using multi-center three-dimensional (3D) neuroimaging datasets from 83 centers using 23 different scanners.ResultsOur results show that HarmonyTM improved PD classification accuracy from 72% to 76% and reduced (unwanted) scanner classification accuracy from 53% to 30% in the TM setup.ConclusionHarmonyTM is a method tailored for harmonizing 3D neuroimaging data within the TM approach, aiming to minimize shortcut learning in distributed setups. This prevents the disease classifier from leveraging scanner-specific details to classify patients with or without PD—a key aspect for deploying ML models for clinical applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="winder_challenges_2024" class="col-sm-8"> <div class="title">Challenges and Potential of Artificial Intelligence in Neuroradiology</div> <div class="author"> Anthony J. Winder, <em>Emma A. M. Stanley</em>, Jens Fiehler, and Nils D. Forkert </div> <div class="periodical"> <em>Clinical Neuroradiology</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/s00062-024-01382-7" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Artificial intelligence (AI) has emerged as a transformative force in medical research and is garnering increased attention in the public consciousness. This represents a critical time period in which medical researchers, healthcare providers, insurers, regulatory agencies, and patients are all developing and shaping their beliefs and policies regarding the use of AI in the healthcare sector. The successful deployment of AI will require support from all these groups. This commentary proposes that widespread support for medical AI must be driven by clear and transparent scientific reporting, beginning at the earliest stages of scientific research.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Book</abbr> </div> <div id="stanley_neuroethics_2023" class="col-sm-8"> <div class="title">Neuroethics considerations for precision medicine and machine learning in neurodevelopmental disorders</div> <div class="author"> <em>Emma A. M. Stanley</em>, Nils D. Forkert, and Sarah J. MacEachern </div> <div class="periodical"> <em>In Developments in Neuroethics and Bioethics</em>, Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/bs.dnb.2023.05.002" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Precision medicine approaches, including machine learning methodologies, are increasingly being applied to research involving individuals with neurodevelopmental disorders (NDDs), which is beginning to impact clinical care for this patient population. However, there are multiple neuroethical issues that should be considered when applying precision medicine and machine learning methodologies to research and clinical work involving individuals with NDDs. This chapter will provide an overview of precision medicine and machine learning and, using examples from the literature, highlight key neuroethical issues that need to be considered when applying precision medicine and machine learning methodologies to research and clinical work involving individuals with NDDs. Neuroethical issues reviewed in this chapter include equitable access, fairness and bias, data privacy, transparency, and sociotechnical considerations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SPIE</abbr> </div> <div id="souza_analysis_2023" class="col-sm-8"> <div class="title">An analysis of intensity harmonization techniques for Parkinson’s multi-site MRI datasets</div> <div class="author"> Raissa Souza, <em>Emma A. M. Stanley</em>, Milton Camacho, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>In Medical Imaging 2023: Computer-Aided Diagnosis</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1117/12.2653948" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Parkinson’s disease (PD) is the second most common neurodegenerative disease affecting 2-3% of the population over 65 years of age. Considerable research has investigated the benefit of using neuroimaging to improve PD diagnosis. However, it is challenging for medical experts to manually identify the subtle differences associated with PD in such complex data. It has been shown that machine learning models can achieve human-like accuracies for many computer-aided diagnosis applications. However, model performance usually depends on the amount and diversity of training data available, whereas most Parkinson’s disease classification models were trained on rather small datasets. Training data size and diversity can be increased by curating multi-site datasets. However, this may also increase biological and non-biological variances due to differences in participant cohorts, scanners, and data acquisition protocols. Thus, data harmonization is important to reduce those variances and enable the models to focus primarily on the patterns associated with PD. This work compares intensity harmonization techniques on 1796 MRI scans from twelve studies. Our results show that a histogram matching approach does not improve classification accuracy (78%) compared to the model trained on unharmonized data (baseline). However, it reduces the disparity between sensitivity and specificity from 81% and 73% to 77% and 79%, respectively. Moreover, combining histogram matching and least squares mean tissue intensity harmonization methods outperform the baseline model (accuracy of 74% compared to 67%) for an independent test set. Finally, our analysis considering sex (male, female) and groups (PD, healthy) shows that models trained on harmonized data exhibited reduced performance disparities between groups, which may be interpreted as a form of bias mitigation.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI</abbr> </div> <div id="stanley_flexible_2023" class="col-sm-8"> <div class="title">A Flexible Framework for Simulating and Evaluating Biases in Deep Learning-Based Medical Image Analysis</div> <div class="author"> <em>Emma A. M. Stanley</em>, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>In Medical Image Computing and Computer Assisted Intervention – MICCAI 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-43895-0_46" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Despite the remarkable advances in deep learning for medical image analysis, it has become evident that biases in datasets used for training such models pose considerable challenges for a clinical deployment, including fairness and domain generalization issues. Although the development of bias mitigation techniques has become ubiquitous, the nature of inherent and unknown biases in real-world medical image data prevents a comprehensive understanding of algorithmic bias when developing deep learning models and bias mitigation methods. To address this challenge, we propose a modular and customizable framework for bias simulation in synthetic but realistic medical imaging data. Our framework provides complete control and flexibility for simulating a range of bias scenarios that can lead to undesired model performance and shortcut learning. In this work, we demonstrate how this framework can be used to simulate morphological biases in neuroimaging data for disease classification with a convolutional neural network as a first feasibility analysis. Using this case example, we show how the proportion of bias in the disease class and proximity between disease and bias regions can affect model performance and explainability results. The proposed framework provides the opportunity to objectively and comprehensively study how biases in medical image data affect deep learning pipelines, which will facilitate a better understanding of how to responsibly develop models and bias mitigation methods for clinical use. Code is available at github.com/estanley16/SimBA.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI WS</abbr> </div> <div id="souza_relationship_2023" class="col-sm-8"> <div class="title">On the Relationship Between Open Science in Artificial Intelligence for Medical Imaging and Global Health Equity</div> <div class="author"> Raissa Souza, <em>Emma A. M. Stanley</em>, and Nils D. Forkert </div> <div class="periodical"> <em>In Clinical Image-Based Procedures, Fairness of AI in Medical Imaging, and Ethical and Philosophical Issues in Medical Imaging</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-45249-9_28" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Artificial intelligence (AI) holds tremendous promise for medical image analysis and computer-aided diagnosis, but various challenges must be addressed to enable its widespread adoption and impact in patient care. Open science, specifically through open-source code and public databases, brings multiple benefits to the progress of AI in medical imaging. It is expected to facilitate research output sharing, promote collaboration among researchers, improve the reproducibility of findings, and foster innovation. However, it is important to recognize that the current state of open-source research, particularly with respect to the large, public datasets commonly used in medical imaging AI, is inherently centered around high-income countries (HIC) and privileged populations. Low- and middle-income countries (LMIC) often face several limitations in contributing to and benefiting from open science research in this domain, such as inadequate digital infrastructure, limited funding for research and development, and a scarcity of healthcare and data science professionals. This may lead to further global disparities in health equity as AI-based clinical decision support systems continue to be implemented in practice. While transfer learning and distributed learning hold promise in addressing some challenges related to limited and non-public data in LMIC, practical obstacles arise when dealing with small, lower-quality datasets, resource constraints, and the need for tailored local implementation of these models. In this commentary, we explore the relationship between open-source models and public medical imaging data repositories in the context of transfer learning and distributed learning, specifically considering their implications for global health equity.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Journal</abbr> </div> <div id="stanley_fairness-related_2022" class="col-sm-8"> <div class="title">Fairness-related performance and explainability effects in deep learning models for brain image analysis</div> <div class="author"> <em>Emma A. M. Stanley</em>, Matthias Wilms, Pauline Mouches, and Nils D. Forkert </div> <div class="periodical"> <em>Journal of Medical Imaging</em>, Aug 2022 </div> <div class="periodical"> Publisher: SPIE </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1117/1.JMI.9.6.061102" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Purpose: Explainability and fairness are two key factors for the effective and ethical clinical implementation of deep learning-based machine learning models in healthcare settings. However, there has been limited work on investigating how unfair performance manifests in explainable artificial intelligence (XAI) methods, and how XAI can be used to investigate potential reasons for unfairness. Thus, the aim of this work was to analyze the effects of previously established sociodemographic-related confounders on classifier performance and explainability methods.Approach: A convolutional neural network (CNN) was trained to predict biological sex from T1-weighted brain MRI datasets of 4547 9- to 10-year-old adolescents from the Adolescent Brain Cognitive Development study. Performance disparities of the trained CNN between White and Black subjects were analyzed and saliency maps were generated for each subgroup at the intersection of sex and race.Results: The classification model demonstrated a significant difference in the percentage of correctly classified White male (90.3 % ± 1.7 % ) and Black male (81.1 % ± 4.5 % ) children. Conversely, slightly higher performance was found for Black female (89.3 % ± 4.8 % ) compared with White female (86.5 % ± 2.0 % ) children. Saliency maps showed subgroup-specific differences, corresponding to brain regions previously associated with pubertal development. In line with this finding, average pubertal development scores of subjects used in this study were significantly different between Black and White females (p \textless 0.001) and males (p \textless 0.001).Conclusions: We demonstrate that a CNN with significantly different sex classification performance between Black and White adolescents can identify different important brain regions when comparing subgroup saliency maps. Importance scores vary substantially between subgroups within brain structures associated with pubertal development, a race-associated confounder for predicting sex. We illustrate that unfair models can produce different XAI results between subgroups and that these results may explain potential reasons for biased performance.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MICCAI WS</abbr> </div> <div id="stanley_disproportionate_2022" class="col-sm-8"> <div class="title">Disproportionate Subgroup Impacts and Other Challenges of Fairness in Artificial Intelligence for Medical Image Analysis</div> <div class="author"> <em>Emma A. M. Stanley</em>, Matthias Wilms, and Nils D. Forkert </div> <div class="periodical"> <em>In Ethical and Philosophical Issues in Medical Imaging, Multimodal Learning and Fusion Across Scales for Clinical Decision Support, and Topological Data Analysis for Biomedical Imaging</em>, Aug 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-23223-7_2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Fairness in artificial intelligence (AI) for medical image analysis is a key factor for preventing new or exacerbated healthcare disparities as the use of automated decision-making tools in medicine increases. However, bias mitigation strategies to achieve group fairness have appreciable shortcomings, which may pose ethical limitations in clinical settings. In this work, we study a well-defined case example of a deep learning-based medical image analysis model exhibiting unfairness between racial subgroups. Specifically, with the task of sex classification using tabulated data from 6,276 T1-weighted brain magnetic resonance imaging (MRI) scans of 9–10 year old adolescents, we investigate how adversarial debiasing for equalized odds between White and Black subgroups affects performance of other structured and intersectional subgroups. Although the debiasing process was successful in reducing classification performance disparities between White and Black subgroups, accuracies for the highest performing subgroups were substantially degraded and disproportionate impacts on performance were seen when considering intersections of sex, race, and socioeconomic status. These results highlight one of the several challenges when attempting to define and achieve algorithmic fairness, particularly in medical imaging applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SPIE</abbr> </div> <div id="stanley_fully_2022" class="col-sm-8"> <div class="title">A fully convolutional neural network for explainable classification of attention deficit hyperactivity disorder</div> <div class="author"> <em>Emma A. M. Stanley</em>, Deepthi Rajashekar, Pauline Mouches, Matthias Wilms, Kira Plettl, and Nils D. Forkert </div> <div class="periodical"> <em>In Medical Imaging 2022: Computer-Aided Diagnosis</em>, Aug 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Emma Stanley. Last updated: November 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>