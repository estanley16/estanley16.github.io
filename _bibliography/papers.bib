---
---


@article{stanley_fairness-related_2022,
	title = {Fairness-related performance and explainability effects in deep learning models for brain image analysis},
	volume = {9},
	issn = {2329-4302, 2329-4310},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-9/issue-6/061102/Fairness-related-performance-and-explainability-effects-in-deep-learning-models/10.1117/1.JMI.9.6.061102.full},
	doi = {10.1117/1.JMI.9.6.061102},
	abstract = {Purpose: Explainability and fairness are two key factors for the effective and ethical clinical implementation of deep learning-based machine learning models in healthcare settings. However, there has been limited work on investigating how unfair performance manifests in explainable artificial intelligence (XAI) methods, and how XAI can be used to investigate potential reasons for unfairness. Thus, the aim of this work was to analyze the effects of previously established sociodemographic-related confounders on classifier performance and explainability methods.Approach: A convolutional neural network (CNN) was trained to predict biological sex from T1-weighted brain MRI datasets of 4547 9- to 10-year-old adolescents from the Adolescent Brain Cognitive Development study. Performance disparities of the trained CNN between White and Black subjects were analyzed and saliency maps were generated for each subgroup at the intersection of sex and race.Results: The classification model demonstrated a significant difference in the percentage of correctly classified White male (90.3 \% ± 1.7 \% ) and Black male (81.1 \% ± 4.5 \% ) children. Conversely, slightly higher performance was found for Black female (89.3 \% ± 4.8 \% ) compared with White female (86.5 \% ± 2.0 \% ) children. Saliency maps showed subgroup-specific differences, corresponding to brain regions previously associated with pubertal development. In line with this finding, average pubertal development scores of subjects used in this study were significantly different between Black and White females (p {\textless} 0.001) and males (p {\textless} 0.001).Conclusions: We demonstrate that a CNN with significantly different sex classification performance between Black and White adolescents can identify different important brain regions when comparing subgroup saliency maps. Importance scores vary substantially between subgroups within brain structures associated with pubertal development, a race-associated confounder for predicting sex. We illustrate that unfair models can produce different XAI results between subgroups and that these results may explain potential reasons for biased performance.},
	number = {6},
	urldate = {2022-10-05},
	journal = {Journal of Medical Imaging},
	author = {Stanley, Emma A. M. and Wilms, Matthias and Mouches, Pauline and Forkert, Nils D.},
	month = aug,
	year = {2022},
	note = {Publisher: SPIE},
	pages = {061102},
  abbr={Journal},
  selected={true},
}

@inproceedings{stanley_disproportionate_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Disproportionate {Subgroup} {Impacts} and {Other} {Challenges} of {Fairness} in {Artificial} {Intelligence} for {Medical} {Image} {Analysis}},
	isbn = {978-3-031-23223-7},
	doi = {10.1007/978-3-031-23223-7_2},
	abstract = {Fairness in artificial intelligence (AI) for medical image analysis is a key factor for preventing new or exacerbated healthcare disparities as the use of automated decision-making tools in medicine increases. However, bias mitigation strategies to achieve group fairness have appreciable shortcomings, which may pose ethical limitations in clinical settings. In this work, we study a well-defined case example of a deep learning-based medical image analysis model exhibiting unfairness between racial subgroups. Specifically, with the task of sex classification using tabulated data from 6,276 T1-weighted brain magnetic resonance imaging (MRI) scans of 9–10 year old adolescents, we investigate how adversarial debiasing for equalized odds between White and Black subgroups affects performance of other structured and intersectional subgroups. Although the debiasing process was successful in reducing classification performance disparities between White and Black subgroups, accuracies for the highest performing subgroups were substantially degraded and disproportionate impacts on performance were seen when considering intersections of sex, race, and socioeconomic status. These results highlight one of the several challenges when attempting to define and achieve algorithmic fairness, particularly in medical imaging applications.},
	language = {en},
	booktitle = {Ethical and {Philosophical} {Issues} in {Medical} {Imaging}, {Multimodal} {Learning} and {Fusion} {Across} {Scales} for {Clinical} {Decision} {Support}, and {Topological} {Data} {Analysis} for {Biomedical} {Imaging}},
	publisher = {Springer Nature Switzerland},
	author = {Stanley, Emma A. M. and Wilms, Matthias and Forkert, Nils D.},
	editor = {Baxter, John S. H. and Rekik, Islem and Eagleson, Roy and Zhou, Luping and Syeda-Mahmood, Tanveer and Wang, Hongzhi and Hajij, Mustafa},
	year = {2022},
	keywords = {Medical image analysis, Algorithmic fairness, Bias mitigation, Computer-aided diagnosis, Adversarial debiasing},
	pages = {14--25},
	abbr={MICCAI WS},
  selected={true},
}

@incollection{stanley_neuroethics_2023,
	title = {Neuroethics considerations for precision medicine and machine learning in neurodevelopmental disorders},
	url = {https://www.sciencedirect.com/science/article/pii/S2589295923000024},
	abstract = {Precision medicine approaches, including machine learning methodologies, are increasingly being applied to research involving individuals with neurodevelopmental disorders (NDDs), which is beginning to impact clinical care for this patient population. However, there are multiple neuroethical issues that should be considered when applying precision medicine and machine learning methodologies to research and clinical work involving individuals with NDDs. This chapter will provide an overview of precision medicine and machine learning and, using examples from the literature, highlight key neuroethical issues that need to be considered when applying precision medicine and machine learning methodologies to research and clinical work involving individuals with NDDs. Neuroethical issues reviewed in this chapter include equitable access, fairness and bias, data privacy, transparency, and sociotechnical considerations.},
	urldate = {2023-08-14},
	booktitle = {Developments in {Neuroethics} and {Bioethics}},
	publisher = {Academic Press},
	author = {Stanley, Emma A. M. and Forkert, Nils D. and MacEachern, Sarah J.},
	month = jun,
	year = {2023},
	doi = {10.1016/bs.dnb.2023.05.002},
	keywords = {Machine learning, Precision medicine, Neurodevelopmental disorders},
  abbr={Book},
}

@inproceedings{stanley_fully_2022,
	title = {A fully convolutional neural network for explainable classification of attention deficit hyperactivity disorder},
	volume = {12033},
	booktitle = {Medical {Imaging} 2022: {Computer}-{Aided} {Diagnosis}},
	publisher = {SPIE},
	author = {Stanley, Emma AM and Rajashekar, Deepthi and Mouches, Pauline and Wilms, Matthias and Plettl, Kira and Forkert, Nils D.},
	year = {2022},
	pages = {310--315},
  abbr={SPIE},
}

@inproceedings{souza_analysis_2023,
	title = {An analysis of intensity harmonization techniques for {Parkinson}’s multi-site {MRI} datasets},
	volume = {12465},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/12465/124652B/An-analysis-of-intensity-harmonization-techniques-for-Parkinsons-multi-site/10.1117/12.2653948.full},
	doi = {10.1117/12.2653948},
	abstract = {Parkinson’s disease (PD) is the second most common neurodegenerative disease affecting 2-3\% of the population over 65 years of age. Considerable research has investigated the benefit of using neuroimaging to improve PD diagnosis. However, it is challenging for medical experts to manually identify the subtle differences associated with PD in such complex data. It has been shown that machine learning models can achieve human-like accuracies for many computer-aided diagnosis applications. However, model performance usually depends on the amount and diversity of training data available, whereas most Parkinson’s disease classification models were trained on rather small datasets. Training data size and diversity can be increased by curating multi-site datasets. However, this may also increase biological and non-biological variances due to differences in participant cohorts, scanners, and data acquisition protocols. Thus, data harmonization is important to reduce those variances and enable the models to focus primarily on the patterns associated with PD. This work compares intensity harmonization techniques on 1796 MRI scans from twelve studies. Our results show that a histogram matching approach does not improve classification accuracy (78\%) compared to the model trained on unharmonized data (baseline). However, it reduces the disparity between sensitivity and specificity from 81\% and 73\% to 77\% and 79\%, respectively. Moreover, combining histogram matching and least squares mean tissue intensity harmonization methods outperform the baseline model (accuracy of 74\% compared to 67\%) for an independent test set. Finally, our analysis considering sex (male, female) and groups (PD, healthy) shows that models trained on harmonized data exhibited reduced performance disparities between groups, which may be interpreted as a form of bias mitigation.},
	urldate = {2023-08-14},
	booktitle = {Medical {Imaging} 2023: {Computer}-{Aided} {Diagnosis}},
	publisher = {SPIE},
	author = {Souza, Raissa and Stanley, Emma A. M. and Camacho, Milton and Wilms, Matthias and Forkert, Nils D.},
	month = apr,
	year = {2023},
	pages = {570--576},
  abbr={SPIE},
}

@inproceedings{stanley_flexible_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Flexible} {Framework} for {Simulating} and {Evaluating} {Biases} in {Deep} {Learning}-{Based} {Medical} {Image} {Analysis}},
	isbn = {978-3-031-43895-0},
	doi = {10.1007/978-3-031-43895-0_46},
	abstract = {Despite the remarkable advances in deep learning for medical image analysis, it has become evident that biases in datasets used for training such models pose considerable challenges for a clinical deployment, including fairness and domain generalization issues. Although the development of bias mitigation techniques has become ubiquitous, the nature of inherent and unknown biases in real-world medical image data prevents a comprehensive understanding of algorithmic bias when developing deep learning models and bias mitigation methods. To address this challenge, we propose a modular and customizable framework for bias simulation in synthetic but realistic medical imaging data. Our framework provides complete control and flexibility for simulating a range of bias scenarios that can lead to undesired model performance and shortcut learning. In this work, we demonstrate how this framework can be used to simulate morphological biases in neuroimaging data for disease classification with a convolutional neural network as a first feasibility analysis. Using this case example, we show how the proportion of bias in the disease class and proximity between disease and bias regions can affect model performance and explainability results. The proposed framework provides the opportunity to objectively and comprehensively study how biases in medical image data affect deep learning pipelines, which will facilitate a better understanding of how to responsibly develop models and bias mitigation methods for clinical use. Code is available at github.com/estanley16/SimBA.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Stanley, Emma A. M. and Wilms, Matthias and Forkert, Nils D.},
	editor = {Greenspan, Hayit and Madabhushi, Anant and Mousavi, Parvin and Salcudean, Septimiu and Duncan, James and Syeda-Mahmood, Tanveer and Taylor, Russell},
	year = {2023},
	pages = {489--499},
	abbr={MICCAI},
}

@inproceedings{souza_relationship_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Relationship} {Between} {Open} {Science} in {Artificial} {Intelligence} for {Medical} {Imaging} and {Global} {Health} {Equity}},
	isbn = {978-3-031-45249-9},
	doi = {10.1007/978-3-031-45249-9_28},
	abstract = {Artificial intelligence (AI) holds tremendous promise for medical image analysis and computer-aided diagnosis, but various challenges must be addressed to enable its widespread adoption and impact in patient care. Open science, specifically through open-source code and public databases, brings multiple benefits to the progress of AI in medical imaging. It is expected to facilitate research output sharing, promote collaboration among researchers, improve the reproducibility of findings, and foster innovation. However, it is important to recognize that the current state of open-source research, particularly with respect to the large, public datasets commonly used in medical imaging AI, is inherently centered around high-income countries (HIC) and privileged populations. Low- and middle-income countries (LMIC) often face several limitations in contributing to and benefiting from open science research in this domain, such as inadequate digital infrastructure, limited funding for research and development, and a scarcity of healthcare and data science professionals. This may lead to further global disparities in health equity as AI-based clinical decision support systems continue to be implemented in practice. While transfer learning and distributed learning hold promise in addressing some challenges related to limited and non-public data in LMIC, practical obstacles arise when dealing with small, lower-quality datasets, resource constraints, and the need for tailored local implementation of these models. In this commentary, we explore the relationship between open-source models and public medical imaging data repositories in the context of transfer learning and distributed learning, specifically considering their implications for global health equity.},
	language = {en},
	booktitle = {Clinical {Image}-{Based} {Procedures},  {Fairness} of {AI} in {Medical} {Imaging}, and {Ethical} and {Philosophical} {Issues} in {Medical} {Imaging}},
	publisher = {Springer Nature Switzerland},
	author = {Souza, Raissa and Stanley, Emma A. M. and Forkert, Nils D.},
	editor = {Wesarg, Stefan and Puyol Antón, Esther and Baxter, John S. H. and Erdt, Marius and Drechsler, Klaus and Oyarzun Laura, Cristina and Freiman, Moti and Chen, Yufei and Rekik, Islem and Eagleson, Roy and Feragen, Aasa and King, Andrew P. and Cheplygina, Veronika and Ganz-Benjaminsen, Melani and Ferrante, Enzo and Glocker, Ben and Moyer, Daniel and Petersen, Eikel},
	year = {2023},
	pages = {289--300},
  abbr={MICCAI WS},
}

@article{souza_identifying_2024,
	title = {Identifying biases in a multicenter {MRI} database for {Parkinson}'s disease classification: {Is} the disease classifier a secret site classifier?},
	issn = {2168-2208},
	shorttitle = {Identifying biases in a multicenter {MRI} database for {Parkinson}'s disease classification},
	url = {https://ieeexplore.ieee.org/document/10388228},
	doi = {10.1109/JBHI.2024.3352513},
	abstract = {Sharing multicenter imaging datasets can be advantageous to increase data diversity and size but may lead to spurious correlations between site-related biological and non-biological image features and target labels, which machine learning (ML) models may exploit as shortcuts. To date, studies analyzing how and if deep learning models may use such effects as a shortcut are scarce. Thus, the aim of this work was to investigate if site-related effects are encoded in the feature space of an established deep learning model designed for Parkinson's disease (PD) classification based on T1-weighted MRI datasets. Therefore, all layers of the PD classifier were frozen, except for the last layer of the network, which was replaced by a linear layer that was exclusively re-trained to predict three potential bias types (biological sex, scanner type, and originating site). Our findings based on a large database consisting of 1880 MRI scans collected across 41 centers show that the feature space of the established PD model (74\% accuracy) can be used to classify sex (75\% accuracy), scanner type (79\% accuracy), and site location (71\% accuracy) with high accuracies despite this information never being explicitly provided to the PD model during original training. Overall, the results of this study suggest that trained image-based classifiers may use unwanted shortcuts that are not meaningful for the actual clinical task at hand. This finding may explain why many image-based deep learning models do not perform well when applied to data from centers not contributing to the training set},
	urldate = {2024-02-02},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Souza, Raissa and Winder, Anthony and Stanley, Emma A.M. and Vigneshwaran, Vibujithan and Camacho, Milton and Camicioli, Richard and Monchi, Oury and Wilms, Matthias and Forkert, Nils D.},
	year = {2024},
	note = {Conference Name: IEEE Journal of Biomedical and Health Informatics},
	keywords = {Biases, Biological system modeling, Brain modeling, Databases, Deep learning, Diseases, Magnetic resonance imaging, Shortcut learning, Training},
	pages = {1--8},
  abbr={Journal},
}

@article{souza_multi-center_2024,
	title = {A multi-center distributed learning approach for {Parkinson}'s disease classification using the traveling model paradigm},
	volume = {7},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2024.1301997},
	doi = {10.3389/frai.2024.1301997},
	abstract = {Distributed learning is a promising alternative to central learning for machine learning (ML) model training, overcoming data-sharing problems in healthcare. Previous studies exploring federated learning (FL) or the travelling model (TM) setup for medical image-based disease classification often relied on large databases with a limited number of centers or simulated artificial centers, raising doubts about real-world applicability. This study develops and evaluates a convolution neural network (CNN) for Parkinson's disease classification using data acquired by 83 diverse real centers around the world, mostly contributing small training samples. Our 1 Souza et al.approach specifically makes use of the TM setup, which has proven effective in scenarios with limited data availability but has never been used for image-based disease classification. Our findings reveal that TM is effective for training CNN models, even in complex real-world scenarios with variable data distributions. After sufficient training cycles, the TM-trained CNN matches or slightly surpasses the performance of the centrally trained counterpart (AUROC of 83\% vs. 80\%).Our study highlights, for the first time, the effectiveness of TM in 3D medical image classification, especially in scenarios with limited training samples and heterogeneous distributed data. These insights are relevant for situations where ML models are supposed to be trained using data from small or remote medical centers, and rare diseases with sparse cases. The simplicity of this approach enables a broad application to many deep learning tasks, enhancing its clinical utility across various contexts and medical facilities.},
	language = {English},
	urldate = {2024-05-24},
	journal = {Frontiers in Artificial Intelligence},
	author = {Souza, Raissa and Stanley, Emma A. M. and Camacho, Milton and Camicioli, Richard and Monchi, Oury and Ismail, Zahinoor and Wilms, Matthias and Forkert, Nils D.},
	month = feb,
	year = {2024},
	note = {Publisher: Frontiers},
	keywords = {Distributed learning, Federated learning, Multi-center, Parkinson's disease, Travelling model},
  abbr={Journal},
}

@article{stanley_towards_2024,
	title = {Towards objective and systematic evaluation of bias in artificial intelligence for medical imaging},
	issn = {1527-974X},
	url = {https://doi.org/10.1093/jamia/ocae165},
	doi = {10.1093/jamia/ocae165},
	abstract = {Artificial intelligence (AI) models trained using medical images for clinical tasks often exhibit bias in the form of subgroup performance disparities. However, since not all sources of bias in real-world medical imaging data are easily identifiable, it is challenging to comprehensively assess their impacts. In this article, we introduce an analysis framework for systematically and objectively investigating the impact of biases in medical images on AI models.Our framework utilizes synthetic neuroimages with known disease effects and sources of bias. We evaluated the impact of bias effects and the efficacy of 3 bias mitigation strategies in counterfactual data scenarios on a convolutional neural network (CNN) classifier.The analysis revealed that training a CNN model on the datasets containing bias effects resulted in expected subgroup performance disparities. Moreover, reweighing was the most successful bias mitigation strategy for this setup. Finally, we demonstrated that explainable AI methods can aid in investigating the manifestation of bias in the model using this framework.The value of this framework is showcased in our findings on the impact of bias scenarios and efficacy of bias mitigation in a deep learning model pipeline. This systematic analysis can be easily expanded to conduct further controlled in silico trials in other investigations of bias in medical imaging AI.Our novel methodology for objectively studying bias in medical imaging AI can help support the development of clinical decision-support tools that are robust and responsible.},
	urldate = {2024-07-01},
	journal = {Journal of the American Medical Informatics Association},
	author = {Stanley, Emma A M and Souza, Raissa and Winder, Anthony J and Gulve, Vedant and Amador, Kimberly and Wilms, Matthias and Forkert, Nils D},
	month = jun,
	year = {2024},
	pages = {ocae165},
  abbr={Journal},
  selected={true},
}

@article{stanley_where_2025,
	title = {Where, why, and how is bias learned in medical image analysis models? {A} study of bias encoding within convolutional networks using synthetic data},
	volume = {111},
	issn = {2352-3964},
	shorttitle = {Where, why, and how is bias learned in medical image analysis models?},
	url = {https://www.sciencedirect.com/science/article/pii/S2352396424005371},
	doi = {10.1016/j.ebiom.2024.105501},
	abstract = {Background
Understanding the mechanisms of algorithmic bias is highly challenging due to the complexity and uncertainty of how various unknown sources of bias impact deep learning models trained with medical images. This study aims to bridge this knowledge gap by studying where, why, and how biases from medical images are encoded in these models.
Methods
We systematically studied layer-wise bias encoding in a convolutional neural network for disease classification using synthetic brain magnetic resonance imaging data with known disease and bias effects. We quantified the degree to which disease-related information, as well as morphology-based and intensity-based biases were represented within the learned features of the model.
Findings
Although biases were encoded throughout the model, a stronger encoding did not necessarily lead to the model using these biases as a shortcut for disease classification. We also observed that intensity-based effects had a greater influence on shortcut learning compared to morphology-based effects when multiple biases were present.
Interpretation
We believe that these results constitute an important first step towards a deeper understanding of algorithmic bias in deep learning models trained using medical imaging data. This study also showcases the benefits of utilising controlled, synthetic bias scenarios for objectively studying the mechanisms of shortcut learning.
Funding
Alberta Innovates, Natural Sciences and Engineering Research Council of Canada, Killam Trusts, Parkinson Association of Alberta, River Fund at Calgary Foundation, Canada Research Chairs Program.},
	urldate = {2024-12-12},
	journal = {eBioMedicine},
	author = {Stanley, Emma A. M. and Souza, Raissa and Wilms, Matthias and Forkert, Nils D.},
	month = jan,
	year = {2025},
	keywords = {Algorithmic bias, Artificial intelligence, Synthetic data},
	pages = {105501},
	abbr={Journal},
  selected={true},
}

@inproceedings{stanley_assessing_2025,
	address = {Cham},
	title = {Assessing the {Impact} of {Sociotechnical} {Harms} in {AI}-{Based} {Medical} {Image} {Analysis}},
	isbn = {978-3-031-72787-0},
	doi = {10.1007/978-3-031-72787-0_16},
	abstract = {Clinical decision-making and radiology will inevitably be transformed by artificial intelligence (AI) in the coming years. The rapid adoption of AI in this domain, and in other everyday applications, has brought an increased awareness of the potential impacts and negative consequences that may occur throughout the sociotechnical systems that these technologies are implemented in. In this paper, we review and apply a previously published taxonomy of the sociotechnical harms of AI to investigate how these harms could manifest during the development and clinical implementation of AI-based medical image analysis. Through an illustrative case study example on computer-aided diagnosis using brain magnetic resonance imaging, we demonstrate how performing impact assessments of sociotechnical harms can assist in operationalizing the medical ethics principle of non-maleficence, thereby guiding the ethical development and implementation of AI technologies in healthcare.},
	language = {en},
	booktitle = {Ethics and {Fairness} in {Medical} {Imaging}},
	publisher = {Springer Nature Switzerland},
	author = {Stanley, Emma A. M. and Souza, Raissa and Winder, Anthony J. and Wilms, Matthias and Pike, G. Bruce and Dagasso, Gabrielle and Nielsen, Christopher and MacEachern, Sarah J. and Forkert, Nils D.},
	editor = {Puyol-Antón, Esther and Zamzmi, Ghada and Feragen, Aasa and King, Andrew P. and Cheplygina, Veronika and Ganz-Benjaminsen, Melanie and Ferrante, Enzo and Glocker, Ben and Petersen, Eike and Baxter, John S. H. and Rekik, Islem and Eagleson, Roy},
	year = {2025},
	pages = {163--175},
	abbr={MICCAI WS},
}

@inproceedings{vigneshwaran_evaluating_2025,
	title = {Evaluating {Shortcut} {Utilization} in {Deep} {Learning} {Disease} {Classification} through {Counterfactual} {Analysis}},
	url = {https://openreview.net/forum?id=vxSo5TJxlB#discussion},
	abstract = {Although deep learning models can surpass human performance in many medical image analysis tasks, they remain vulnerable to algorithmic shortcuts, where spurious correlations in the data are exploited, which may lead to reduced trust in their predictions/classifications. This issue is especially concerning when models rely on protected attributes (e.g., sex, race, or site) as shortcuts. Such shortcut reliance not only impairs their ability to generalize to unseen datasets but also raises fairness concerns, ultimately undermining their purpose for computer-aided diagnosis. Previous techniques for analyzing protected attributes, such as supervised prediction layer information tests, only highlight the presence of protected attributes in the feature space but do not confirm their role in solving the primary task. Determining the impact of protected attributes as shortcuts is particularly challenging, as it requires knowing how a model would perform without those attributes — a counterfactual scenario typically unattainable in real-world data. As a workaround, researchers have addressed the absence of counterfactuals by generating synthetic datasets with and without protected attributes. In this study, we propose a novel approach to evaluate real-world datasets and determine the extent to which each protected attribute is used as a shortcut in a classification task. Therefore, we define and train a causal generative model to produce causally-grounded counterfactuals, removing protected attributes from activations and allowing us to measure their impact on model performance. Employing T1-weighted MRI data from 9 sites (835 subjects: 426 with Parkinson’s disease (PD) and 409 healthy), we demonstrate that counterfactually removing the 'site' attribute from the penultimate layer of a trained classification model reduced the AUROC for PD classification from 0.74 to 0.65, indicating a 9\% performance improvement achieved by using 'site' as a shortcut. In contrast, counterfactually removing the 'sex' attribute had minimal impact on performance, with only a slight change of 0.004, indicating that 'sex' was not utilized as a shortcut by the classification model. The proposed method offers a robust framework for assessing shortcut utilization in medical image classification, paving the way for improved bias detection and mitigation in medical imaging tasks. The code for this work is available on https://github.com/vibujithan/shortcut-analysis.},
	language = {en},
	urldate = {2025-06-16},
	author = {Vigneshwaran, Vibujithan and Stanley, Emma A. M. and Souza, Raissa and Ohara, Erik and Wilms, Matthias and Forkert, Nils},
	month = jan,
	year = {2025},
  abbr={MIDL},
}

@article{souza_harmonytm_2024,
	title = {{HarmonyTM}: multi-center data harmonization applied to distributed learning for {Parkinson}’s disease classification},
	volume = {11},
	issn = {2329-4302, 2329-4310},
	shorttitle = {{HarmonyTM}},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-11/issue-5/054502/HarmonyTM--multi-center-data-harmonization-applied-to-distributed-learning/10.1117/1.JMI.11.5.054502.full},
	doi = {10.1117/1.JMI.11.5.054502},
	abstract = {PurposeDistributed learning is widely used to comply with data-sharing regulations and access diverse datasets for training machine learning (ML) models. The traveling model (TM) is a distributed learning approach that sequentially trains with data from one center at a time, which is especially advantageous when dealing with limited local datasets. However, a critical concern emerges when centers utilize different scanners for data acquisition, which could potentially lead models to exploit these differences as shortcuts. Although data harmonization can mitigate this issue, current methods typically rely on large or paired datasets, which can be impractical to obtain in distributed setups.ApproachWe introduced HarmonyTM, a data harmonization method tailored for the TM. HarmonyTM effectively mitigates bias in the model’s feature representation while retaining crucial disease-related information, all without requiring extensive datasets. Specifically, we employed adversarial training to “unlearn” bias from the features used in the model for classifying Parkinson’s disease (PD). We evaluated HarmonyTM using multi-center three-dimensional (3D) neuroimaging datasets from 83 centers using 23 different scanners.ResultsOur results show that HarmonyTM improved PD classification accuracy from 72\% to 76\% and reduced (unwanted) scanner classification accuracy from 53\% to 30\% in the TM setup.ConclusionHarmonyTM is a method tailored for harmonizing 3D neuroimaging data within the TM approach, aiming to minimize shortcut learning in distributed setups. This prevents the disease classifier from leveraging scanner-specific details to classify patients with or without PD—a key aspect for deploying ML models for clinical applications.},
	number = {5},
	urldate = {2025-07-28},
	journal = {Journal of Medical Imaging},
	author = {Souza, Raissa and Stanley, Emma A. M. and Gulve, Vedant and Moore, Jasmine and Kang, Chris and Camicioli, Richard and Monchi, Oury and Ismail, Zahinoor and Wilms, Matthias and Forkert, Nils D.},
	month = sep,
	year = {2024},
	note = {Publisher: SPIE},
	pages = {054502},
	file = {Souza et al_2024_HarmonyTM.pdf:/Users/emmastanley/Documents/Zotero/Souza et al_2024_HarmonyTM.pdf:application/pdf},
  abbr={Journal},
}

@inproceedings{stanley_does_2025,
	title = {Does a diffusion-based generative classifier avoid shortcut learning in medical image analysis? {An} initial investigation using synthetic neuroimaging data},
	volume = {13411},
	shorttitle = {Does a diffusion-based generative classifier avoid shortcut learning in medical image analysis?},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13411/134110J/Does-a-diffusion-based-generative-classifier-avoid-shortcut-learning-in/10.1117/12.3046811.full},
	doi = {10.1117/12.3046811},
	abstract = {Powerful deep learning-based classification models have led to impressive performance gains of computer-aided diagnosis systems in medical imaging over the last decade. However, the training of such discriminative models usually requires a significant amount of labelled training images that are representative of the data the model will see after (clinical) deployment. Acquiring such data is often highly challenging and costly in medical image analysis setups, which may lead to so-called shortcut learning where the model learns a relationship in the data that is not causally related to the actual classification task. Biased models are known to be less robust and may act unfairly when applied to data from subpopulations underrepresented in the training data, resulting in ethical challenges when deployed clinically. Recent results from the machine learning domain have shown that generative classifiers (i.e., a generative model repurposed as a classifier) might be less prone to this shortcut learning behavior than standard discriminative classifiers. Motivated by those previous results, this work provides the first-ever analysis of the shortcut learning behavior of a diffusion model-based generative classifier for medical image analysis. Our investigation relies on a state-of-the-art framework for synthetic neuroimage data generation with customizable biases. The initial results obtained using this data showcase that while our generative classifier still relies on shortcuts to render its decision to a certain extent, the degree of shortcut learning is substantially reduced compared to a standard discriminative classifier - a convolutional neural network - trained on identical data. We believe that these results highlight the advantages generative classifiers could potentially have on a path towards more trustworthy and equitable AI in medical image analysis.},
	urldate = {2025-07-29},
	booktitle = {Medical {Imaging} 2025: {Imaging} {Informatics}},
	publisher = {SPIE},
	author = {Stanley, Emma A. M. and Forkert, Nils D. and Wilms, Matthias},
	month = apr,
	year = {2025},
	pages = {94--99},
  abbr={SPIE},
}

@inproceedings{martinez_explainable_2025,
	title = {Explainable classification of autism in children with a convolutional neural network},
	volume = {13411},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13411/1341105/Explainable-classification-of-autism-in-children-with-a-convolutional-neural/10.1117/12.3047066.full},
	doi = {10.1117/12.3047066},
	abstract = {Autism is a complex neurodevelopmental condition that influences how individuals interact, communicate, and behave. Although its prevalence is high, it can be challenging to make an early diagnosis, mainly due to the gradual onset patterns of its symptoms. Artificial intelligence (AI) models using magnetic resonance imaging (MRI) can support the diagnosis of autism in children by detecting complex disease-related brain patterns that are not obvious to human experts. The purpose of this study was to develop and evaluate an explainable deep learning (DL) model to support the diagnosis of autism in children and to identify the most important brain regions for the classification task. For the development and evaluation of the proposed DL model, we used 452 T1-weighted structural magnetic resonance images of individuals aged 9 to 11 years from the Autism Brain Imaging Data Exchange I and II (ABIDE I and II) databases. Using this data sample, a convolutional neural network was trained to classify neurologically typical children and autistic children (360 used for training / 46 images for validation / 46 images for testing). The results based on the images used as the test set showed that the proposed deep learning method achieves an overall accuracy of 71.74\%, with a sensitivity of 73.91\% and a specificity of 70.83\%. The corresponding saliency voxel attribution maps were computed, which hihglighted the left transverse temporal gyrus, the left lateral ventricle, the left VI-VII vermal lobules, and the left thalamus as the most important regions for the classification task. These brain regions are consistent with previous studies that identified differences in these areas in autistic individuals. To our knowledge, this is the first study that aims to classify autism in children aged 9-11 years using a deep learning approach based on structural MRI data in combination with artificial intelligence explainability techniques to identify relevant brain regions for this task.},
	urldate = {2025-07-31},
	booktitle = {Medical {Imaging} 2025: {Imaging} {Informatics}},
	publisher = {SPIE},
	author = {Martinez, Garazi Casillas and Winder, Anthony and Stanley, Emma A. M. and Souza, Raissa and Wilms, Matthias and Estes, Myka and MacEachern, Sarah J. and Forkert, Nils D.},
	month = apr,
	year = {2025},
	pages = {15--21},
  abbr={SPIE},
}

@article{mcavoy_brain_2025,
	title = {Brain {Aging} in {Patients} {With} {Cardiovascular} {Disease} {From} the {UK} {Biobank}},
	volume = {46},
	copyright = {© 2025 The Author(s). Human Brain Mapping published by Wiley Periodicals LLC.},
	issn = {1097-0193},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.70252},
	doi = {10.1002/hbm.70252},
	abstract = {The brain undergoes complex but normal structural changes during the aging process in healthy adults, whereas deviations from the normal aging patterns of the brain can be indicative of various conditions as well as an increased risk for the development of diseases. The brain age gap (BAG), which is defined as the difference between the chronological age and the machine learning-predicted biological age of an individual, is a promising biomarker for determining whether an individual deviates from normal brain aging patterns. While the BAG has shown promise for various neurological diseases and cardiovascular risk factors, its utility to quantify brain changes associated with diagnosed cardiovascular diseases has not been investigated to date, which is the aim of this study. T1-weighted MRI scans from healthy participants in the UK Biobank were used to train a convolutional neural network (CNN) model for biological brain age prediction. The trained model was then used to quantify and compare the BAGs for all participants in the UK Biobank with known cardiovascular diseases, as well as healthy controls and patients with known neurological diseases for benchmark comparisons. Saliency maps were computed for each individual to investigate whether brain regions used for biological brain age prediction by the CNN differ between groups. The analyses revealed significant differences in BAG distributions for 10 of the 42 sex-specific cardiovascular disease groups investigated compared to healthy participants, indicating disease-specific variations in brain aging. However, no significant differences were found regarding the brain regions used for brain age prediction as determined by saliency maps, indicating that the model mostly relied on healthy brain aging patterns, even in the presence of cardiovascular diseases. Overall, the findings of this work demonstrate that the BAG is a sensitive imaging biomarker to detect differences in brain aging associated with specific cardiovascular diseases. This further supports the theory of the heart–brain axis by exemplifying that many cardiovascular diseases are associated with atypical brain aging.},
	language = {en},
	number = {8},
	urldate = {2025-07-31},
	journal = {Human Brain Mapping},
	author = {Mcavoy, Elizabeth and Stanley, Emma A. M. and Winder, Anthony J. and Wilms, Matthias and Forkert, Nils D.},
	year = {2025},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/hbm.70252},
	pages = {e70252},
  abbr={Journal},
}

@inproceedings{stanley_synthetic_2026,
	address = {Cham},
	title = {Synthetic {Ground} {Truth} {Counterfactuals} for {Comprehensive} {Evaluation} of {Causal} {Generative} {Models} in {Medical} {Imaging}},
	isbn = {978-3-032-04984-1},
	doi = {10.1007/978-3-032-04984-1_52},
	abstract = {Counterfactuals in medical imaging are synthetic representations of how an individual’s medical image might appear under alternative, typically unobservable conditions, which have the potential to address data limitations and enhance interpretability. However, counterfactual images, which can be generated by causal generative models (CGMs), are inherently hypothetical—raising questions of how to properly validate that they are realistic and accurately reflect the intended modifications. A common approach for quantitatively evaluating CGM-generated counterfactuals involves using a discriminative model as a ‘pseudo-oracle’ to assess whether interventions on specific variables are effective. However, this method is not well-suited for in-depth error identification and analysis of CGMs. To address this limitation, we propose to leverage synthetic, ‘ground truth’ counterfactual datasets as a novel approach for debugging and evaluating CGMs. These synthetic datasets enable the computation of global performance metrics and precise localization of CGM failure modes. To further quantify failures, we introduce a novel metric, the Triangulation of Effectiveness and Amplification (TEA), which precisely quantifies the effectiveness of target variable interventions and the additional amplification of unintended effects. We test and validate our evaluation framework on two state-of-the-art CGMs where the results demonstrate the utility of synthetic datasets in identifying failure modes of CGMs, and highlight the potential of the proposed TEA metric as a robust tool for evaluation of their performance. Code and data are available at https://github.com/ucalgary-miplab/TEA.},
	language = {en},
	booktitle = {Medical {Image} {Computing} and {Computer} {Assisted} {Intervention} – {MICCAI} 2025},
	publisher = {Springer Nature Switzerland},
	author = {Stanley, Emma A. M. and Vigneshwaran, Vibujithan and Ohara, Erik Y. and Vamosi, Finn G. and Forkert, Nils D. and Wilms, Matthias},
	editor = {Gee, James C. and Alexander, Daniel C. and Hong, Jaesung and Iglesias, Juan Eugenio and Sudre, Carole H. and Venkataraman, Archana and Golland, Polina and Kim, Jong Hyo and Park, Jinah},
	year = {2026},
	keywords = {brain MRI, causal generative modeling, counterfactuals},
	pages = {541--550},
  abbr={MICCAI},
  selected={true},
}

@inproceedings{stanley_exploring_2026,
	address = {Cham},
	title = {Exploring the {Interplay} of {Label} {Bias} with {Subgroup} {Size} and {Separability}: {A} {Case} {Study} in {Mammographic} {Density} {Classification}},
	isbn = {978-3-032-05870-6},
	shorttitle = {Exploring the {Interplay} of {Label} {Bias} with {Subgroup} {Size} and {Separability}},
	doi = {10.1007/978-3-032-05870-6_8},
	abstract = {Systematic mislabelling affecting specific subgroups (i.e., label bias) in medical imaging datasets represents an understudied issue concerning the fairness of medical AI systems. In this work, we investigated how size and separability of subgroups affected by label bias influence the learned features and performance of a deep learning model. Therefore, we trained deep learning models for binary tissue density classification using the EMory BrEast imaging Dataset (EMBED), where label bias affected separable subgroups (based on imaging manufacturer) or non-separable ‘pseudo-subgroups’. We found that simulated subgroup label bias led to prominent shifts in the learned feature representations of the models. Importantly, these shifts within the feature space were dependent on both the relative size and the separability of the subgroup affected by label bias. We also observed notable differences in subgroup performance depending on whether a validation set with clean labels was used to define the classification threshold for the model. For instance, with label bias affecting the majority separable subgroup, the true positive rate for that subgroup fell from 0.898, when the validation set had clean labels, to 0.518, when the validation set had biased labels. Our work represents a key contribution toward understanding the consequences of label bias on subgroup fairness in medical imaging AI.},
	language = {en},
	booktitle = {Fairness of {AI} in {Medical} {Imaging}},
	publisher = {Springer Nature Switzerland},
	author = {Stanley, Emma A. M. and Mehta, Raghav and Roschewitz, Mélanie and Forkert, Nils D. and Glocker, Ben},
	editor = {Puyol-Antón, Esther and Ferrante, Enzo and Feragen, Aasa and King, Andrew and Cheplygina, Veronika and Ganz-Benjaminsen, Melani and Glocker, Ben and Petersen, Eike and Lee, Heisook},
	year = {2026},
	keywords = {Fairness, Label Bias, Mammography},
	pages = {74--83},
  abbr={MICCAI WS},
  selected={true},
}

@article{winder_challenges_2024,
	title = {Challenges and {Potential} of {Artificial} {Intelligence} in {Neuroradiology}},
	volume = {34},
	issn = {1869-1447},
	url = {https://doi.org/10.1007/s00062-024-01382-7},
	doi = {10.1007/s00062-024-01382-7},
	abstract = {Artificial intelligence (AI) has emerged as a transformative force in medical research and is garnering increased attention in the public consciousness. This represents a critical time period in which medical researchers, healthcare providers, insurers, regulatory agencies, and patients are all developing and shaping their beliefs and policies regarding the use of AI in the healthcare sector. The successful deployment of AI will require support from all these groups. This commentary proposes that widespread support for medical AI must be driven by clear and transparent scientific reporting, beginning at the earliest stages of scientific research.},
	language = {en},
	number = {2},
	urldate = {2025-11-17},
	journal = {Clinical Neuroradiology},
	author = {Winder, Anthony J. and Stanley, Emma AM and Fiehler, Jens and Forkert, Nils D.},
	month = jun,
	year = {2024},
	keywords = {Ischemic stroke, Machine learning, Precision medicine, Predictive analytics, Translational medicine},
	pages = {293--305},
  abbr={Journal},
}

@article{nielsen_assessment_2025,
	title = {Assessment of demographic bias in retinal age prediction machine learning models},
	volume = {8},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1653153/full},
	doi = {10.3389/frai.2025.1653153},
	abstract = {The retinal age gap, defined as the difference between the predicted retinal age and chronological age, is an emerging biomarker for many eye conditions and even non-ocular diseases. Machine learning (ML) models are commonly used for retinal age prediction. However, biases in ML models may lead to unfair predictions for some demographic groups, potentially exacerbating health disparities. This retrospective cross-sectional study evaluated demographic biases related to sex and ethnicity in retinal age prediction models using retinal imaging data (color fundus photography [CFP], optical coherence tomography [OCT], and combined CFP + OCT) from 9,668 healthy individuals (mean age 56.8 years; 52\% female) in the UK Biobank. The RETFound foundation model was fine-tuned to predict retinal age, and bias was assessed by comparing mean absolute error (MAE) and retinal age gaps across demographic groups. The combined CFP + OCT model achieved the lowest MAE (3.01 years), outperforming CFP-only (3.40 years) and OCT-only (4.37 years) models. Significant sex differences were observed only in the CFP model (p {\textless} 0.001), while significant ethnicity differences appeared only in the OCT model (p {\textless} 0.001). No significant sex/ethnicity differences were observed in the combined model. These results demonstrate that retinal age prediction models can exhibit biases, and that these biases, along with model accuracy, are influenced by the choice of imaging modality (CFP, OCT, or combined). Identifying and addressing sources of bias is essential for safe and reliable clinical implementation. Our results emphasize the importance of comprehensive bias assessments and prospective validation, ensuring that advances in machine learning and artificial intelligence benefit all patient populations.},
	language = {English},
	urldate = {2025-11-17},
	journal = {Frontiers in Artificial Intelligence},
	author = {Nielsen, Christopher and Stanley, Emma A. M. and Wilms, Matthias and Forkert, Nils D.},
	month = oct,
	year = {2025},
	note = {Publisher: Frontiers},
	keywords = {bias, machine learning, multimodal imaging, retinal age prediction, retinal imaging},
  abbr={Journal},
}

@inproceedings{gillett_simulating_2026,
	address = {Cham},
	title = {Simulating {Inter}-observer {Variability} {Across} {Clinical} {Experience} {Levels} for {Brain} {Tumour} {Segmentation}},
	isbn = {978-3-032-08970-0},
	doi = {10.1007/978-3-032-08970-0_8},
	abstract = {Human-AI collaboration is essential for the development of trustworthy deep learning (DL) models for medical image analysis. However, datasets annotated by multiple clinical experts can introduce inter-observer variability, which can then give rise to annotator biases that may be learned by the DL model. Assessment of these biases is often hindered by the limited availability of multi-observer annotations for the same datasets. To address this limitation, we present a novel simulation framework that generates realistic variations in annotated segmentations to mimic inter-observer differences across simulated human experts with varying experience levels. Using brain tumour segmentation as a representative case study, we simulated three observer labels to train DL models. Our results show that DL models learn observer-specific annotation styles. For example, models trained on the data from a simulated senior radiologist with a tendency to under-segment the tumour tissue achieved higher performance than those trained on over-segmented ones. Inter-observer agreement was not strictly correlated with experience levels nor downstream DL model performance, demonstrating the complexity of annotation biases. Additionally, datasets with single ground-truth labels may mask important differences from learned annotation bias and over- or underestimate model performance. Human-AI collaboration, although necessary for medical imaging tasks, can introduce biases that negatively affect model segmentation performance and may undermine fairness, trust, and transparency. Our study takes an essential step toward understanding these risks and provides insights that support the development of human–AI collaborative systems designed for real-world clinical applicability.},
	language = {en},
	booktitle = {Human-{AI} {Collaboration}},
	publisher = {Springer Nature Switzerland},
	author = {Gillett, Haley and Stanley, Emma A. M. and Souza, Raissa and Wilms, Matthias and Forkert, Nils D.},
	editor = {Guo, Xiaoqing and Jin, Yueming and Lamdouar, Hala and Men, Qianhui and Ouyang, Cheng and Sahu, Manish and Vedula, S. Swaroop},
	year = {2026},
	keywords = {Annotator Bias, Human Factors, Human-AI Collaboration, Inter-observer Variability, Label Noise, Medical Image Segmentation},
	pages = {81--90},
  abbr={MICCAI WS},
}

@article{souza_self-supervised_2025,
	title = {Self-supervised identification and elimination of harmful datasets in distributed machine learning for medical image analysis},
	volume = {8},
	copyright = {2025 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-025-01499-0},
	doi = {10.1038/s41746-025-01499-0},
	abstract = {Distributed learning enables collaborative machine learning model training without requiring cross-institutional data sharing, thereby addressing privacy concerns. However, local quality control variability can negatively impact model performance while systematic human visual inspection is time-consuming and may violate the goal of keeping data inaccessible outside acquisition centers. This work proposes a novel self-supervised method to identify and eliminate harmful data during distributed learning model training fully-automatically. Harmful data is defined as samples that, when included in training, increase misdiagnosis rates. The method was tested using neuroimaging data from 83 centers for Parkinson’s disease classification with simulated inclusion of a few harmful data samples. The proposed method reliably identified harmful images, with centers providing only harmful datasets being easier to identify than single harmful images within otherwise good datasets. While only evaluated using neuroimaging data, the presented method is application-agnostic and presents a step towards automated quality control in distributed learning.},
	language = {en},
	number = {1},
	urldate = {2025-11-17},
	journal = {npj Digital Medicine},
	author = {Souza, Raissa and Stanley, Emma A. M. and Winder, Anthony J. and Kang, Chris and Amador, Kimberly and Ohara, Erik Y. and Dagasso, Gabrielle and Camicioli, Richard and Monchi, Oury and Ismail, Zahinoor and Wilms, Matthias and Forkert, Nils D.},
	month = feb,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomedical engineering, Computational science},
	pages = {104},
  abbr={Journal},
}

@article{moore_towards_2025,
	title = {Towards realistic simulation of disease progression in the visual cortex with {CNNs}},
	volume = {15},
	copyright = {2025 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-025-89738-y},
	doi = {10.1038/s41598-025-89738-y},
	abstract = {Convolutional neural networks (CNNs) and mammalian visual systems share architectural and information processing similarities. We leverage these parallels to develop an in-silico CNN model simulating diseases affecting the visual system. This model aims to replicate neural complexities in an experimentally controlled environment. Therefore, we examine object recognition and internal representations of a CNN under neurodegeneration and neuroplasticity conditions simulated through synaptic weight decay and retraining. This approach can model neurodegeneration from events like tau accumulation, reflecting cognitive decline in diseases such as posterior cortical atrophy, a condition that can accompany Alzheimer’s disease and primarily affects the visual system. After each degeneration iteration, we retrain unaffected synapses to simulate ongoing neuroplasticity. Our results show that with significant synaptic decay and limited retraining, the model’s representational similarity decreases compared to a healthy model. Early CNN layers retain high similarity to the healthy model, while later layers are more prone to degradation. The results of this study reveal a progressive decline in object recognition proficiency, mirroring posterior cortical atrophy progression. In-silico modeling of neurodegenerative diseases can enhance our understanding of disease progression and aid in developing targeted rehabilitation and treatments.},
	language = {en},
	number = {1},
	urldate = {2025-11-17},
	journal = {Scientific Reports},
	author = {Moore, Jasmine A. and Kang, Chris and Vigneshwaran, Vibujithan and Stanley, Emma A. M. and Memon, Ashar and Wilms, Matthias and Forkert, Nils D.},
	month = feb,
	year = {2025},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational neuroscience, Dementia, Network models},
	pages = {6099},
  abbr={Journal},
}

@inproceedings{souza_sites_2025,
	address = {Cham},
	title = {Do {Sites} {Benefit} {Equally} from {Distributed} {Learning} in {Medical} {Image} {Analysis}?},
	isbn = {978-3-031-72787-0},
	doi = {10.1007/978-3-031-72787-0_12},
	abstract = {Artificial intelligence (AI) has the potential to make medical image analysis more accessible to healthcare institutions worldwide. However, when trained on multi-site datasets, models may excel with data from certain institutions but struggle with others, even when exposed to their training data. This emphasizes the importance of investigating whether all sites benefit from AI models, especially within distributed learning setups. Distributed learning methods allow access to broader and more diverse datasets from multiple sites during training. In this context, the travelling model (TM) paradigm has demonstrated superior performance in limited data scenarios, making it particularly relevant in low-resource settings. This work investigates whether all sites can benefit from TM development and identifies the factors influencing performance disparities. Specifically, a Parkinson’s disease (PD) database comprising 1,817 neuroimaging datasets from 83 different sites is utilized to investigate how site-specific and participant-specific factors influence the performance of TM in classifying PD. Therefore, we analyze the false positive rate (FPR) and false negative rate (FNR) to identify the characteristics contributing to misdiagnosis. Our findings reveal disparities in benefits across sites, with class imbalance emerging as the major factor influencing FPR and FNR, especially for sites with more PD cases. This research underscores the urgency of a rigorous analysis of a model’s behaviour in distributed setups to detect misdiagnosis risks and encourage developers to evaluate and optimize models beyond overall accuracy. Thus, comprehensive analyses of this type can help pave the way for the development of more equitable AI-based medical imaging models.},
	language = {en},
	booktitle = {Ethics and {Fairness} in {Medical} {Imaging}},
	publisher = {Springer Nature Switzerland},
	author = {Souza, Raissa and Stanley, Emma A. M. and Camicioli, Richard and Monchi, Oury and Ismail, Zahinoor and Wilms, Matthias and Forkert, Nils D.},
	editor = {Puyol-Antón, Esther and Zamzmi, Ghada and Feragen, Aasa and King, Andrew P. and Cheplygina, Veronika and Ganz-Benjaminsen, Melanie and Ferrante, Enzo and Glocker, Ben and Petersen, Eike and Baxter, John S. H. and Rekik, Islem and Eagleson, Roy},
	year = {2025},
	keywords = {Distributed learning, Health equity, Site benefit, Travelling model},
	pages = {119--128},
	abbr={MICCAI WS},
}

